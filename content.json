{"meta":{"title":"Rango's blog","subtitle":null,"description":"一个菜鸟码农","author":"Rango","url":"http://yoursite.com"},"pages":[{"title":"","date":"2017-11-01T05:24:30.000Z","updated":"2017-11-01T05:24:30.000Z","comments":true,"path":"404.html","permalink":"http://yoursite.com/404.html","excerpt":"","text":""},{"title":"about","date":"2017-11-01T05:16:50.000Z","updated":"2017-11-01T05:16:50.000Z","comments":true,"path":"about/index.html","permalink":"http://yoursite.com/about/index.html","excerpt":"","text":""},{"title":"categories","date":"2017-11-01T05:16:21.000Z","updated":"2017-11-01T05:18:20.000Z","comments":true,"path":"categories/index.html","permalink":"http://yoursite.com/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2017-11-01T04:17:10.000Z","updated":"2017-11-01T04:52:37.000Z","comments":true,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"python 爬取图片","slug":"python-ys","date":"2017-11-01T08:33:20.000Z","updated":"2017-11-01T08:42:06.000Z","comments":true,"path":"2017/11/01/python-ys/","link":"","permalink":"http://yoursite.com/2017/11/01/python-ys/","excerpt":"从V2EX 看到有人在爬煎蛋的获赞数高的妹子图，刚好这几天刚接触python,就从那位发贴者的网站爬取他抓取的图片数据，并以txt文件保存至本地。","text":"从V2EX 看到有人在爬煎蛋的获赞数高的妹子图，刚好这几天刚接触python,就从那位发贴者的网站爬取他抓取的图片数据，并以txt文件保存至本地。 爬虫入口文件12345678910111213141516171819202122232425262728import url_managerimport html_downloadimport html_parserimport data_outputclass Spider(object): def __init__(self): self.urls = url_manager.UrlManager() self.html_download = html_download.HtmlDownload() self.html_parser = html_parser.HtmlParser() self.html_output = data_output.OutPut() def craw(self, root_url, page_url): self.urls.add_new_url(root_url) i = 1 while self.urls.has_new_urls(): new_url = self.urls.get_new_url() print 'craw %d : %s' % (i, new_url) cont = self.html_download.download(new_url) url, data = self.html_parser.paser(page_url, cont) self.urls.add_new_url(url) self.html_output.create(data) # i = i + 1if __name__ == '__main__': root_url = '***' page_url = '**' obj_Spider = Spider() obj_Spider.craw(root_url, page_url) 数据采集器12345678910111213import urllib2class HtmlDownload(object): def download(self, url): if url is None: return None request = urllib2.Request(url) request.add_header('User-Agent', 'Mozilla/5/0') response = urllib2.urlopen(request) if response.getcode() != 200: return None return response.read() 数据处理器123456789101112131415161718192021222324252627282930313233343536import reimport urlparsefrom bs4 import BeautifulSoupclass HtmlParser(object): def __init__(self): self.old_request_url = set() def paser(self, page_url, cont): if page_url is None or cont is None: return soup = BeautifulSoup(cont, 'html.parser', from_encoding='utf-8') new_url = self._get_new_url(page_url, soup) data = self._get_data(page_url, soup) return new_url, data def _get_data(self, page_url, soup): links = soup.find_all('img') list =[] for link in links: url = link.attrs.get('data-original') if url is not None: new_full_url = urlparse.urljoin(page_url, url) list.append(new_full_url) return list def _get_new_url(self, page_url, soup): links = soup.find_all('a', href=re.compile(r\"/meizi/rank/\")) for link in links: if link['href'] not in self.old_request_url: new_url = link['href'] self.old_request_url.add(link['href']) new_full_url = urlparse.urljoin(page_url, new_url) return new_full_url 数据保存123456789class OutPut(object): def create(self, data): for i in data: links = open(\"data.txt\") if i not in links: fout = open('data.txt', 'a') fout.write(i+'\\n') fout.close()","categories":[{"name":"python","slug":"python","permalink":"http://yoursite.com/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"http://yoursite.com/tags/python/"}]},{"title":"妹子图","slug":"meizi","date":"2017-11-01T05:54:30.000Z","updated":"2017-11-01T07:16:27.000Z","comments":true,"path":"2017/11/01/meizi/","link":"","permalink":"http://yoursite.com/2017/11/01/meizi/","excerpt":"","text":"","categories":[{"name":"python","slug":"python","permalink":"http://yoursite.com/categories/python/"}],"tags":[{"name":"python","slug":"python","permalink":"http://yoursite.com/tags/python/"}]},{"title":"iterm2 保存ssh密码","slug":"article171024","date":"2017-10-24T09:23:22.000Z","updated":"2017-11-01T04:10:06.000Z","comments":true,"path":"2017/10/24/article171024/","link":"","permalink":"http://yoursite.com/2017/10/24/article171024/","excerpt":"使用脚本连接 1.创建脚本文件 vim ~/.ssh/ssh1","text":"使用脚本连接 1.创建脚本文件 vim ~/.ssh/ssh1 2.粘贴内容 1234567891011#!/usr/bin/expect -f set user &lt;用户名&gt; set host &lt;ip地址&gt; set password &lt;密码&gt; set timeout -1 spawn ssh $user@$host expect &quot;*assword:*&quot; send &quot;$password\\r&quot; interact expect eof 3.打开iterm2-&gt;Profiles 新建连接 commod 处执行 expect ~/.ssh/ssh1 1执行连接是先手动连接一次ssh 保存seesion key 复制 ssh1```sudo cd ~/.ssh/cp ssh1 ssh2","categories":[],"tags":[{"name":"Mac","slug":"Mac","permalink":"http://yoursite.com/tags/Mac/"}]}]}